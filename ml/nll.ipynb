{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relation between Negative Log Likelihood, Cross-Entropy and Mean Squared Error\n",
    "\n",
    "NLL, CE and MSE are very similar concepts, often used in ML as losses to train a ML model for inference. \n",
    "Even though they have different names, they are referring to the same concept.\n",
    "\n",
    "## Negative Log Likelihood\n",
    "\n",
    "* Likelihood: originates from bayesian statistics and describes the probability of model parameters $\\theta$ given fixed (training) data $x$. Usually when training a model, our training data $x$ is fixed (invariant) and we try find the best values for $\\theta$ such that the $x$ is most probable. Thus $\\mathcal{L}(\\theta|x) = P(x | \\theta)$, the likelihood is a function over the parameter values of $\\theta$ given data $x$ which corresponds to the probability of the data $x$ given $\\theta$.\n",
    "\n",
    "* Log Likelihood: we take the log function over the likelihood function due to numerical precisions issues in floating points calculations. Also the log function has the neat property to turn a product into a sum: $log(a * b) = log(a) + log(b)$. This helps solving the floating points issues, as well as some issues arising from differentiating through chain rule (vanishing gradients)\n",
    "\n",
    "* Negative Log Likelihood: in machine learning, it is common to minimize a loss. Since we want to find the best parameter values of a model given a fixed dataset $\\mathcal{L} = \\underset{\\theta}{\\argmax}log(P(x|\\theta))$, this is corresponds to maximizing the log probability of the data $x$ given the parameter values $\\theta$. To treat this now as a loss and a minimization problem, we simply take the negative of it. Minimizing the negative log likelihood is the same as maximizing the log likelihood.\n",
    "\n",
    "## Cross-Entropy (CE)\n",
    "\n",
    "$H(p,q) = - \\sum_{x \\in \\chi}p(x)\\log q(x)$\n",
    "\n",
    "(where does \"log\" comes in?)\n",
    "\n",
    "## Mean-Squared-Error (MSE)\n",
    "\n",
    "MSE is just a special case of Cross-Entropy, where we assume a guassian distribution over the data $x$ with a fixed variance of $\\sigma^2(x_i) = 1$. If we consider this and apply these assumptions to the CE, we get ($x_i$ being the different data points and $y_i$ the corresponding label):\n",
    "$$NLL=CE=\\sum_{i} \\frac{\\log(\\sigma^2(x_i))}{2} + \\frac{(y_i - \\mu(x_i))^2}{2\\sigma^2(x_i)} = \\frac{1}{2} \\sum_{i} (y_i - \\mu(x_i))^2 = MSE$$ \n",
    "So essentially, MSE is a special case of Cross-Entropy where we optimize for $\\mu$ keeping $\\sigma^2 = 1$. CE is the (mathematically) same as the negative log likelihood like in the previous section.\n",
    "\n",
    "\n",
    "## References\n",
    "* [Likelihood function](https://en.wikipedia.org/wiki/Likelihood_function)\n",
    "* [NLL to MSE](https://towardsdatascience.com/mse-is-cross-entropy-at-heart-maximum-likelihood-estimation-explained-181a29450a0b)\n",
    "* [MSE to NLL](https://fairyonice.github.io/Create-a-neural-net-with-a-negative-log-likelihood-as-a-loss.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
